{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PASCAL\\\\Student_Performance_Prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PASCAL\\\\Student_Performance_Prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    preprocessor_path: Path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from studentPerformance.constants import *\n",
    "from studentPerformance.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            preprocessor_path = config.preprocessor_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.studentPerformance.logger import logging\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        try:\n",
    "            numerical_columns = ['reading_score', 'writing_score']\n",
    "            categorical_columns = ['gender', 'race_ethnicity', \n",
    "                                   'parental_level_of_education', \n",
    "                                   'lunch', 'test_preparation_course']\n",
    "\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"one_hot_encoder\", OneHotEncoder()),\n",
    "                    (\"scaler\", StandardScaler(with_mean=False))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "                    (\"cat_pipelines\", cat_pipeline, categorical_columns)\n",
    "                ],\n",
    "                remainder=\"drop\"  # Ignore any columns not explicitly specified\n",
    "            )\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in get_data_transformer_object: {str(e)}\")\n",
    "\n",
    "    def initiate_data_transformation(self):\n",
    "        try:\n",
    "            train_data_path = 'artifacts/data_ingestion/unzipped_data/train_data.csv'  # Replace with the actual path to your train data file\n",
    "            test_data_path = 'artifacts/data_ingestion/unzipped_data/test_data.csv'  # Replace with the actual path to your test data file\n",
    "\n",
    "            logging.info(\"Read train and test data completed\")\n",
    "\n",
    "            logging.info(\"Obtaining preprocessing object\")\n",
    "\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "\n",
    "            target_column_name = \"math_score\"\n",
    "\n",
    "            train_df = pd.read_csv(train_data_path)\n",
    "            test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            logging.info(\"Applying preprocessing object on training dataframe and testing dataframe.\")\n",
    "\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            train_arr = np.c_[\n",
    "                input_feature_train_arr, np.array(target_feature_train_df)\n",
    "            ]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            # Save preprocessing object\n",
    "            preprocessing_obj_file = os.path.join(\"artifacts\", 'data_transformation', 'preprocessing_obj.pkl')\n",
    "            with open(preprocessing_obj_file, 'wb') as file:\n",
    "                pickle.dump(preprocessing_obj, file)\n",
    "\n",
    "            logging.info(\"Saved preprocessing object.\")\n",
    "            logging.info(\"Transformation of the data is completed\")\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                preprocessing_obj_file\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in initiate_data_transformation: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 02:30:21,336: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-07-24 02:30:21,340: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-07-24 02:30:21,342: INFO: common: created directory at: artifacts]\n",
      "[2023-07-24 02:30:21,400: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-07-24 02:30:21,401: INFO: 2947525798: Read train and test data completed]\n",
      "[2023-07-24 02:30:21,402: INFO: 2947525798: Obtaining preprocessing object]\n",
      "[2023-07-24 02:30:21,403: INFO: 2947525798: Categorical columns: ['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']]\n",
      "[2023-07-24 02:30:21,404: INFO: 2947525798: Numerical columns: ['reading_score', 'writing_score']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 02:30:21,510: INFO: 2947525798: Applying preprocessing object on training dataframe and testing dataframe.]\n",
      "[2023-07-24 02:30:21,609: INFO: 2947525798: Saved preprocessing object.]\n",
      "[2023-07-24 02:30:21,610: INFO: 2947525798: Transformation of the data is completed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_transformation_config)\n",
    "    data_transformation.initiate_data_transformation()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
